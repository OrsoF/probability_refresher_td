\documentclass{article}

%%% Packages %%%

\usepackage[margin=1in]{geometry}
\usepackage{exercise}
\renewcommand{\ExerciseHeader}{%
  \par\noindent
  \textbf{\large \ExerciseName \, \ExerciseHeaderNB\ExerciseHeaderTitle\ExerciseHeaderOrigin}%
  \par\nopagebreak\medskip
}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{url, hyperref}

%%% Commands %%%

\newcommand{\proba}[1]{\mathbb{P}[#1]}
\newcommand{\var}[1]{\operatorname{Var}[#1]}
\newcommand{\esperance}[1]{\mathbb{E}[#1]}
\newcommand{\limitinftyn}{\xrightarrow[n \to \infty]{}}
\newcommand{\indicator}[1]{\mathbb{1}_{#1}}
\newcommand{\R}{\mathbb{R}}

%%% Solutions %%%

\newtheorem{solution}{Solution}
\newif\ifhidesolutions
% \hidesolutionstrue %decommenter pour cacher les SOLUTIONS

\ifhidesolutions
  \usepackage{environ}
  \NewEnviron{hide}{}
  \let\solution\hide
  \let\endsolution\endhide
\fi

%%% Title %%%

\title{PC 2 - Probability distributions}
\author{}
\date{}

%%% Document %%%

\begin{document}

\begin{flushleft}
  \textbf{Probability Refresher} \hfill \textbf{September 2023} \\
  \textbf{Master X-HEC} \hfill \textbf{C. Deslandes, A. Loison,} \\
  É\textbf{cole Polytechnique} \hfill \textbf{D. Métivier \& T. Rebafka}
\end{flushleft}

{\let\newpage\relax\maketitle}
\vspace{-1.3cm}
\hrule

\vspace{0.5cm}

\begin{Exercise} (Uniform distribution). Let $X$ be a random variable with uniform distribution on $[0,1]$. We define $Y=\min (X, 1-X)$ and $Z=\max (X, 1-X)$. Determine the distributions of $Y$ and $Z$. Compute $\mathbb{E}[Y Z]$.
\end{Exercise}

\begin{solution}
  La variable al\'{e}atoire $Y$ prend ses valeurs dans $[1/2,1]$ et pour tout $t\in[1/2,1]$,
  \[
    F_Y(t) = \mathbb{P}(U\leq t,1-U\leq t) = \mathbb{P}(U\leq t,U\geq 1-t)
    = t - (1-t) = 2t-1
  \]
  donc $Y$ suit la loi uniforme sur $[1/2,1]$. On remarque que $X = 1-Y$ et on en
  d\'{e}duit que $X$ suit la loi uniforme sur $[0,1/2]$. Pour calculer
  $\mathbb{E}[XY]$, on remarque que $XY = U(1-U)$ et donc
  \[
    \mathbb{E}[XY]
    = \mathbb{E}[U(1-U)]
    = \int_0^{1}\!(t-t^2)\,dt
      = [t^2/2-t^3/3]_0^{1}
    = 1/2-1/3=1/6.
  \]
\end{solution}

\begin{Exercise}
  One says that $X \in (0,+\infty)$ follows the $\log$-normal distribution if $\log (X) \sim \mathcal{N}(0,1)$. What is the density of $X$ ?
\end{Exercise}

\begin{solution}
  The density of $\log (X)$ is a log normal distribution :
  $$f_{\log (X)}(x) = \frac{1}{x\sqrt{2\pi}} \exp{\left( -\frac{(\log(x))^2}{2} \right)}$$
\end{solution}

\begin{Exercise}
  Consider a random variable $X$ having exponential distribution with parameter 1 . Let $a>0$ be a positive real number.
  \begin{enumerate}
    \item Compute the cumulative distribution function of $Y=\min (X, a)$. Plot the
          function.

    \item What can you say about the existence of a density for the distribution of $Y$ ?

    \item Compute $\mathbb{E}[Y]$. Hint: Use $Y=X \mathbb{1}_{X \leq a}+a
            \mathbb{1}_{X>a}$.
  \end{enumerate}
\end{Exercise}



\begin{Exercise} Let $V$ be a random variable with uniform distribution on $[0, \pi / 2]$. Define the random variable $W=\sin (V)$.
  \begin{enumerate}
    \item Determine the distributions of $W$.
    \item How does the distribution of $W$ change when $V$ has uniform distribution on $[0, \pi]$ ?
  \end{enumerate}
\end{Exercise}

\begin{Exercise} (Cauchy distribution). Let $X$ be a random variable with Cauchy distribution whose density is given by $f(x)=\left(\pi\left(1+x^{2}\right)\right)^{-1}$. Determine the distribution of $1 / X$ using a change of variables.
\end{Exercise}

\begin{solution}
  Soit $f:\mathbb R\longrightarrow \mathbb R$ continue born\'ee. On a
  \begin{align*}
    \mathbb E[f(\frac1X)] & =\int_\mathbb R f(\frac1x) \frac{1}{\pi(1+x^2)} dx.
  \end{align*}
  On a envie de faire le changement $u=1/x$ mais pas bijectif sur $\mathbb R!$ on scinde en deux

  \begin{align*}
    \mathbb E[f(\frac1X)] & =\int_0^{+\infty} f(\frac1x) \frac{1}{\pi(1+x^2)} dx+ \int_{-\infty}^0 f(\frac1x) \frac{1}{\pi(1+x^2)} dx.
  \end{align*}
  On pose la variable $u=1/x$ donc $du=-u^2 dx$ ainsi
  \begin{align*}
    \int_0^{+\infty} f(\frac1x) \frac{1}{\pi(1+x^2)} dx & =\int_0^{+\infty} f(u)\frac{1}{u^2}  \frac{1}{\pi(1+u^{-2})} du \\
                                                        & =\int_0^{+\infty} f(u) \frac{1}{\pi(1+u^2)} du.
  \end{align*}
  De plus en faisant $u=1/x$ dans l'integrale sur $\mathbb R^-$ on a de m\^eme

  \begin{align*}
    \int_{-\infty}^0 f(\frac1x) \frac{1}{\pi(1+x^2)} dx & =\int_{-\infty}^0 f(u) \frac{1}{\pi(1+u^2)} du.
  \end{align*}
  Donc $\frac1X$ a m\^eme loi que $X$.
\end{solution}

\begin{Exercise}
  Let $p>0$ and an integer $n$ such that $n>p$. Consider random variables $Y_{n}$ such that $n Y_{n}$ has a geometric distribution $\operatorname{Geo}\left(\frac{p}{n}\right)$ with parameter $\frac{p}{n}$. Show that the characteristic function of $Y_{n}$ tends to the characteristic function of an exponentially distributed random variable with parameter $p$.
\end{Exercise}

\begin{solution}
  We have :
  \begin{align*}
    \phi_{Y_n}(t) & = \esperance{e^{itY_n}}                              \\
                  & = \sum_{k = 0}^\infty p e^{itk} (1-p)^{k+1}          \\
                  & = p (1-p) \sum_{k = 0}^\infty (\exp{(it)}.(1-p))^{k} \\
                  & = \frac{p (1-p) }{1-(1-p).\exp{(it)}}
  \end{align*}
  And if $X \sim Exp(p)$ :
  \begin{align*}
    \phi_{X}(t) & = \esperance{e^{itX}}                                      \\
                & = \sum_{k = 0}^\infty p e^{itk} e^{-p} \frac{p^k}{k!}      \\
                & = p e^{-p} \sum_{k = 0}^\infty \frac{(\exp{(it)}.p)^k}{k!} \\
                & = p e^{-p} \exp{((\exp{(it)}.p))}
  \end{align*}
\end{solution}

\begin{Exercise} Let $\alpha>1$ be fixed. Consider the random variable $X$ with density given by
  $$
    f(x)=c_{\alpha} x^{-\alpha} \mathbb{1}_{x \geq 1}
  $$
  \begin{enumerate}
    \item Determine the constant $c_{\alpha}$.

    \item For which values of $p$ we have $X$ belongs to $L^{p}$ ?

  \end{enumerate}
\end{Exercise}

\begin{solution}
  \begin{enumerate}
    \item Necessarily, $\int_\R f(x) = 1$. So :
          \begin{align*}
                 & \int_\R c_{\alpha} x^{-\alpha} \mathbb{1}_{x \geq 1} dx = 1
            \iff & c_{\alpha} \int_1^\infty x^{-\alpha} dx = 1
            \iff & c_{\alpha} = \frac{-1}{\frac{1}{1-\alpha}}                  \\
            \iff & c_{\alpha} = \alpha -1
          \end{align*}
    \item We have :
          \begin{align*}
            X \in L^p & \iff \esperance{X^p} < \infty                                  \\
                      & \iff \int_\R x^p x^{-\alpha} \mathbb{1}_{x \geq 1} dx < \infty
                      & \iff \int_1^\infty x^{p-\alpha} dx < \infty                    \\
                      & \iff p - \alpha < -1
          \end{align*}
          To understant the last step, see \url{https://boilley.ovh/cours/integrale-generalisee.html}.
          And finally the nessary and sufficient condition is $p - \alpha < -1$.
  \end{enumerate}
\end{solution}

\begin{Exercise} Let $X$ and $Y$ be two independent random variables such that $X$ (resp. $Y$ ) has geometric distribution with parameter $p$ (resp. $q$ ).
  \begin{enumerate}
    \item Compute $\mathbb{P}(X>n)$ for any $n \in \mathbb{N}$.
    \item What is the distribution of the random variable $Z=\min (X, Y)$ ?
  \end{enumerate}
\end{Exercise}

\begin{solution}
  \begin{enumerate}
    \item We have :
          \begin{align*}
            \proba{X > n} & = \sum_{k = n+1}^\infty p (1-p)^k \\
                          & = (1-p)^{n+1}
          \end{align*}
          using a formula for the sum of geometric terms.
    \item Let determine the value of $\proba{\min(X, Y) = k}$.
          \begin{align*}
            \proba{\min(X, Y) = k} & = \proba{(X = k \cap Y > k) \cup (X > k \cap Y = k) \cup (X = k \cap Y = k)}     \\
                                   & = \proba{X = k \cap Y > k} + \proba{X > k \cap Y = k} + \proba{X = k \cap Y = k} \\
                                   & = p (1-p)^k (1-q)^{k+1} + (1-p)^{k+1} q (1-q)^{k} + p (1-p)^k q (1-q)^{k}        \\
                                   & = (1-p)^k (1-q)^k (p + q - pq)
          \end{align*}
  \end{enumerate}
\end{solution}

\begin{Exercise} Assume that $X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$.
  \begin{enumerate}
    \item Show that $Y=(X-\mu) / \sigma$ has standard normal distribution
          $\mathcal{N}(0,1)$.
    \item Compute $\mathbb{E}[|Y|]$ and $\mathbb{E}\left[Y^{2019}\right]$.
  \end{enumerate}
\end{Exercise}

\begin{solution}
  \begin{enumerate}
    \item If $Y=(X-\mu)/\sigma$, then :
          \begin{align*}
            \proba{a \leq Y \leq b} & = \proba{a \leq \frac{X-\mu}{\sigma} \leq b}                                                                     \\
                                    & = \proba{a\sigma + \mu \leq X \leq b\sigma + \mu}                                                                \\
                                    & = \int_{a\sigma + \mu}^{b\sigma + \mu} \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{{(x-\mu)}^2}{2\sigma^2}} dx      \\
                                    & = \int_a^b \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy \text{ with change of variable } y = \frac{x-\mu}{\sigma}
          \end{align*}
          Therefore, $Y$ follows a normal distribution.
    \item Let compute $\esperance{|Y|}$.
          \begin{align*}
            \esperance{|Y|} & = \frac{1}{\sqrt{2\pi}} \int_\R |x| e^{\frac{-x^2}{2}}            \\
                            & = \frac{1}{\sqrt{2\pi}}.2. \int_{\R^+} x e^{\frac{-x^2}{2}}       \\
                            & = \sqrt{\frac{2}{\pi}} \left[ e^{\frac{-x^2}{2}} \right]_0^\infty \\
                            & = \sqrt{\frac{2}{\pi}}
          \end{align*}
  \end{enumerate}
\end{solution}

\end{document}