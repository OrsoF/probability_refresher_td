\begin{center}
  \section*{Random vectors}
\end{center}

%%% Exercise 2 %%%

\begin{Exercise}
  Denote

  \[
    f(x, y)=c \mathrm{e}^{-x} \mathbb{1}_{|y| \leq x}
  \]

  \begin{enumerate}
    \item Find $c$ such that $f$ is a probability density function of a pair $(X, Y)$ of
          random variables.

    \item Compute the marginal distributions of $X$ and $Y$.

    \item Conclude on the independence of $X$ and $Y$.

  \end{enumerate}
\end{Exercise}

%%% Solution 2 %%%

\begin{solution}
  \begin{enumerate}
    \item We have:
          \begin{align*}
            f \text{ is a density } & \iff \int_{\R^2} c \mathrm{e}^{-x} \mathbb{1}_{|y| \leq x} d(x,y)= 1           \\
                                    & \iff \int_{\R} \int_{\R^+} c \mathrm{e}^{-x} \mathbb{1}_{|y| \leq x} dx dy = 1 \\
                                    & \iff \int_{\R^+} 2 x c \mathrm{e}^{-x} dx= 1                                   \\
                                    & \iff 2c \int_{\R^+} x \mathrm{e}^{-x} dx= 1                                    \\
                                    & \iff 2c = 1 \iff c = \frac{1}{2}
          \end{align*}
    \item Moreover,
          \begin{align*}
            f_X(x) & = x e^{-x}
          \end{align*}
          And
          \[
            f_Y(y) =  \frac{1}{2} e^{-y}
          \]
    \item We finally have:
          \[
            f(x, y) \neq f_X(x) f_Y(y)
          \]
          and the random variables therefore are not independents.
  \end{enumerate}
\end{solution}

%%% Exercise 3 %%%

\begin{Exercise}
  Let $X$ and $Y$ be two random variables taking their values in $\mathbb{N}$. Consider the joint probability mass function of $(X, Y)$ given by
  \[
    \proba{(X = i) \cap (Y = j)} = \frac{a}{2^{i+j}}, i, j \in \mathbb{N}, a \in \mathbb{R} .
  \]
  \begin{enumerate}
    \item Compute $a$.
    \item Give the marginal distributions of $X$ and $Y$.
    \item Are $X$ and $Y$ independent?
  \end{enumerate}
\end{Exercise}

%%% Solution 3 %%%

\begin{solution}
  \begin{enumerate}
    \item We have:
          \[
            \sum_{i, j = 0}^\infty \frac{a}{2^{i+j}} = a {\left( \sum_{i =
                  0}^\infty \frac{1}{2^i} \right)}^2 = a.2.2 = 4a
          \]
          Therefore, $4a = 1$ and finally $a = \frac{1}{4}$.
    \item We have:
          \begin{align*}
            \proba{X = i} & = \sum_{j = 0}^\infty \proba{(X = i) \cap (Y = j)} \\
                          & = \sum_{j = 0}^\infty \frac{1}{4.2^i.2^j}          \\
                          & = \frac{1}{2^{i+1}}
          \end{align*}
          In the same way:
          \[ \proba{Y = i} = \frac{1}{2^{i+1}} \]
    \item We have:
          \[
            \proba{(X = i) \cap (Y = j)} = \frac{1}{2^{i+j+2}} =
            \left(\frac{1}{2^{i+1}} \right) \left(\frac{1}{2^{j+1}} \right) = \proba{X = i}
            \proba{Y = j}
          \]
          And the random variables are therefore independents.
  \end{enumerate}
\end{solution}

%%% Exercise 4 %%%

\begin{Exercise}
  Denote

  \[
    f(x, y)=a\left(x^{2}+y^{2}\right) \mathbb{1}_{(x, y) \in {[-1,1]}^{2}} .
  \]

  \begin{enumerate}
    \item Find $a$ such that $f$ is a probability density. We denote $(X, Y)$ the pair of
          random variables with joint distribution $f$.

    \item Compute the marginal distributions of $X$ and $Y$.

    \item Compute the covariance of $X$ and $Y$.

    \item Are $X$ and $Y$ independent?

  \end{enumerate}
\end{Exercise}

%%% Solution 4 %%%

% To Do

\begin{solution}
  \begin{enumerate}
    \item We have
          \begin{align*}
            \int_{{[-1, 1]}^2} x^2 + y^2 dx dy & = 2 \int_{{[-1, 1]}^2} x^2 ~ dx ~ dy \\
                                               & = 4 \int_{{[-1, 1]}^2} x^2 ~ dx      \\
                                               & = 8 \int_0^1 x^2 ~ dx                \\
                                               & = \frac{8}{3}
          \end{align*}
          Therefore $a = \frac{3}{8}$.
    \item We have
          \begin{align*}
            f_X(x) & = a \int_{-1}^1 x^2 + y^2 dy                               \\
                   & = a \left( \int_{-1}^1 x^2 dy + \int_{-1}^1 y^2 dy \right) \\
                   & = 2 a (x^2 + \frac{1}{3})                                  \\
                   & = \frac{3}{4} (x^2 + \frac{1}{3})                          \\
                   & = \frac{3x^2 + 1}{4}
          \end{align*}
          And by symmetry $f_Y = f_X$.
    \item We have $$\esperance{X} = \int_{-1}^1 x \frac{3x^2 + 1}{4} dx = 0 $$ and
          \begin{align*}
            \cov{X} & = \esperance{X^2}                           \\
                    & = \int_{-1}^1 x^2  \frac{3x^2 + 1}{4} dx    \\
                    & = \frac{3}{4} \int_{-1}^1 x^4 + \frac{1}{2} \\
                    & = \frac{8}{10}
          \end{align*}
          and same for $Y$.
    \item As clearly $f \neq f_X \cdot f_Y$, $X$ and $Y$ are not independent.
  \end{enumerate}
\end{solution}

%%% Exercise 5 %%%

\begin{Exercise}
  Let $\mathbf{X}=\left(X_{1}, X_{2}, X_{3}\right)$ be a random vector with the following covariance matrix

  \[
    \operatorname{Cov}(\mathbf{X})=\left(\begin{array}{lll}
        2 & 1 & 3 \\
        1 & 5 & 6 \\
        3 & 6 & 9
      \end{array}\right)
  \]

  \begin{enumerate}
    \item Give the variance of $X_{2}$ and the covariance between $X_{1}$ and $X_{3}$.

    \item Compute the variance of $Z=X_{3}-\alpha_{1} X_{1}-\alpha_{2} X_{2}$ for
          $\alpha_{1}, \alpha_{2} \in \mathbb{R}$.

    \item Deduce that $X_{3}$ is almost surely a linear combination of $X_{1}$ and
          $X_{2}$.

    \item More generally, let $\mathbf{Y}$ be a random vector. Give a necessary and
          sufficient condition on the covariance matrix of $\mathbf{Y}$ ensuring that one
          of the components of $\mathbf{Y}$ is almost surely a linear combination of the
          components of $\mathbf{Y}$.

  \end{enumerate}

\end{Exercise}

%%% Solution 5 %%%

\begin{solution}
  \begin{enumerate}
    \item We have $\var{X_2} = 5$ and $\cov{X_1, X_3} = 3$.
    \item We want to compute $\var{Z}$. Let us note that $Z = X . y$ with $y = \begin{pmatrix} -1 \\ -1 \\ 1 \end{pmatrix}$. Therefore,
          \begin{align*}
            \var{Z} & = \var{X.y}     \\
                    & = y^T \var{X} y \\
                    & =
            \begin{pmatrix}
              -1 & -1 & 1
            \end{pmatrix}
            \begin{pmatrix}
              2 & 1 & 3 \\
              1 & 5 & 6 \\
              3 & 6 & 9
            \end{pmatrix}
            \begin{pmatrix}
              -1 \\ -1 \\ 1
            \end{pmatrix}            \\
                    & =
            \begin{pmatrix}
              -1 \\ -1 \\ 1
            \end{pmatrix}
            \begin{pmatrix}
              0 & 0 & 0
            \end{pmatrix}            \\
                    & = 0
          \end{align*}
          Thus, $\var{Z} = 0$.
    \item Thus, $Z = 0$ almost surely and finally $X_3$ is almost surely a linear combination of $X_1$ and $X_2$.
    \item The necessary and sufficient condition on $\cov{Y}$ is:
          \[
            \exists y\in \R^n, Y \cdot y = 0 \text{ almost surely} \iff \cov{Y} \text{ singular matrix}
          \]
          The proof is quite the same as in Question 2.:
          \begin{align*}
            \exists y\in \R^n, Y \cdot y = 0  \text{ almost surely} & \iff \var{Y \cdot y} = 0 \\
                                                                    & \iff y^T \var{Y} y = 0   \\
          \end{align*}
          As $\var{Y}$ is a symmetric matrix semi-definite positive, it can be written $\var{Y} = \Lambda^T \Lambda$ (Cholesky decomposition). Therefore :
          \begin{align*}
            \exists y\in \R^n, Y \cdot y = 0  \text{ almost surely} & \iff y^T \Lambda^T \Lambda y = 0     \\
                                                                    & \iff \|\Lambda y\|_2^2 = 0           \\
                                                                    & \iff \Lambda y = 0_n                 \\
                                                                    & \iff \Lambda^T \Lambda y = 0_n       \\
                                                                    & \iff \exists y, \var{Y} . y = 0_n    \\
                                                                    & \iff \cov{Y} \text{ singular matrix}
          \end{align*}
  \end{enumerate}
\end{solution}